{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "road_sign_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s8IRw4_hh8Ta"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np \n",
        "import random\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torch.nn.modules.activation import Sigmoid\n",
        "from torch.optim import optimizer\n",
        "\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/trafic_32.zip"
      ],
      "metadata": {
        "id": "N9Mlb-rkjXL-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "BoCmKr27kTCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3350f3-8f0c-4c37-893f-5fd10c497851"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dane"
      ],
      "metadata": {
        "id": "7b8YPVOvmTFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "dataset = ImageFolder(\"/content/trafic_32\", transform=train_transform)\n"
      ],
      "metadata": {
        "id": "Kw8SY9aQmUwH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "validation_split = .2\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
        "                                           sampler=train_sampler, num_workers=2)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=valid_sampler, num_workers=2)"
      ],
      "metadata": {
        "id": "3DCEVKcemgvY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir /content/validation_data\n",
        "!rm /content/validation_data/*\n",
        "\n",
        "for batch, (images, labels) in enumerate(validation_loader):\n",
        "    for cnt, image in enumerate(images):\n",
        "        if batch*batch_size + cnt < 1000:\n",
        "            save_image(image, '/content/validation_data/val_' + str(batch*batch_size + cnt) + '.png')\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "id": "Me7R3fBtFtnP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = dataset.classes"
      ],
      "metadata": {
        "id": "qaT6lg6HAbbl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_len = {}\n",
        "for batch in validation_loader:\n",
        "    for label in batch[1]:\n",
        "        if label.item() not in classes_len:\n",
        "            classes_len[label.item()] = 1\n",
        "        else:\n",
        "            classes_len[label.item()] += 1"
      ],
      "metadata": {
        "id": "yKIqRixuFsNA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "X9sCxHcJmF3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class View(nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super(View, self).__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(*self.shape)"
      ],
      "metadata": {
        "id": "zODXRBugYy_Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HamsterConcurentLayer(nn.Module):\n",
        "    \"\"\"Simple residual conection module\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(HamsterConcurentLayer, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(channels),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_hat = self.layers(x)\n",
        "        x_hat += x\n",
        "        return x_hat"
      ],
      "metadata": {
        "id": "NXJxy4PmVMqr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, cond_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_dim, out_channels=32, stride=(2,2), kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, stride=(2, 2), kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.res_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            HamsterConcurentLayer(input_dim),\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            HamsterConcurentLayer(hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.fc_mean  = nn.Linear(hidden_dim + cond_dim, latent_dim)\n",
        "        self.fc_var   = nn.Linear(hidden_dim + cond_dim, latent_dim)\n",
        "        \n",
        "        self.training = True\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                elif isinstance(m, nn.Conv2d):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "  \n",
        "    def forward(self, x, y=None):\n",
        "        \n",
        "        # x = self.conv_layers(x)\n",
        "        # x = self.res_layers(x)\n",
        "        x = self.layers(x)\n",
        "\n",
        "        if y is not None:\n",
        "            x = torch.cat((x, y), dim=1)\n",
        "        mean     = self.fc_mean(x)\n",
        "        log_var  = self.fc_var(x)  \n",
        "        return mean, log_var"
      ],
      "metadata": {
        "id": "BmyGhBe5knZn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, cond_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Linear(latent_dim + cond_dim, 192),\n",
        "            nn.ReLU(),\n",
        "            View([-1, 3, 8, 8]),\n",
        "            nn.ConvTranspose2d(in_channels = 3, out_channels=128, stride=2, kernel_size=3, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ConvTranspose2d(in_channels = 128, out_channels=64, stride=2, kernel_size=3, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ConvTranspose2d(in_channels = 64, out_channels=3, stride=1, kernel_size=3, padding=1),\n",
        "            nn.Flatten(),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(latent_dim + cond_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.res_layers = nn.Sequential(\n",
        "            HamsterConcurentLayer(latent_dim + cond_dim),\n",
        "            nn.Linear(latent_dim + cond_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            HamsterConcurentLayer(hidden_dim),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "\n",
        "        if y is not None:\n",
        "            x = torch.cat((x, y), dim=1)\n",
        "\n",
        "        # x_hat = self.conv_layers(x)\n",
        "        # x_hat = self.res_layers(x)\n",
        "        x_hat = self.layers(x)\n",
        "        \n",
        "        x_hat = x_hat.view([-1, 3, 32, 32])\n",
        "        return x_hat"
      ],
      "metadata": {
        "id": "m8BIqX7PkofB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE(nn.Module):\n",
        "    \"\"\"Conditional VAE\"\"\"\n",
        "    def __init__(self, x_dim, nclass, hidden_dim, latent_dim, cond_dim):\n",
        "        super(cVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = Encoder(input_dim=x_dim*32*32, cond_dim=cond_dim, hidden_dim=hidden_dim, latent_dim=latent_dim) # if conv layers are used input_dim=x_dim else input_dim=x_dim*32*32\n",
        "        self.decoder = Decoder(latent_dim=latent_dim, cond_dim=cond_dim, hidden_dim = hidden_dim, output_dim = x_dim*32*32)\n",
        "        self.label_embedding = nn.Embedding(nclass, cond_dim)\n",
        "\n",
        "        \n",
        "    def sampling(self, mean, var):\n",
        "        z = torch.randn_like(mean) * var + mean\n",
        "        return z\n",
        "        \n",
        "                \n",
        "    def forward(self, x, y=None):\n",
        "        if y is not None:\n",
        "            y = self.label_embedding(y)\n",
        "        mean, log_var = self.encoder(x, y)\n",
        "        z = self.sampling(mean, torch.exp(0.5 * log_var))\n",
        "        x_hat = self.decoder(z, y)\n",
        "        return x_hat, mean, log_var\n",
        "\n",
        "    def generate(self, z, y=None):\n",
        "        if y is not None:\n",
        "            y = self.label_embedding(y)\n",
        "        x_hat = self.decoder(z, y)\n",
        "        return x_hat"
      ],
      "metadata": {
        "id": "nYaxn9tdk9NZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cVAE(latent_dim=256, cond_dim=128, hidden_dim=1024, x_dim=3, nclass=len(classes)).to(device)"
      ],
      "metadata": {
        "id": "6oqieOrHlDFs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trenowanie"
      ],
      "metadata": {
        "id": "74hOBw_DmJBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss_function(x, x_hat, mean, log_var, b):\n",
        "    MSE = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "    return MSE + b*KLD"
      ],
      "metadata": {
        "id": "UrbsgFSYugvy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frange_cycle_sigmoid(start, stop, n_epoch, n_cycle=4, ratio=0.5):\n",
        "    L = np.ones(n_epoch)\n",
        "    period = n_epoch/n_cycle\n",
        "    step = (stop-start)/(period*ratio)\n",
        "\n",
        "    for c in range(n_cycle):\n",
        "\n",
        "        v , i = start , 0\n",
        "        while v <= stop:\n",
        "            L[int(i+c*period)] = 1.0/(1.0+ np.exp(- (v*12.-6.)))\n",
        "            v += step\n",
        "            i += 1\n",
        "    return L    "
      ],
      "metadata": {
        "id": "5br_4x29XoKc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.99)"
      ],
      "metadata": {
        "id": "LwKutB9Glpdh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "beta_np_cycle = frange_cycle_sigmoid(0.0, 1.0, num_epochs, 4)"
      ],
      "metadata": {
        "id": "KbChT_-RYIwD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    losses_epoch = []\n",
        "    model.train()\n",
        "    for x, label in iter(train_loader):\n",
        "        x = x.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        out, means, log_var = model(x, label)\n",
        "        # print(out.shape)\n",
        "        loss = vae_loss_function(x, out, means, log_var, beta_np_cycle[epoch]) \n",
        "        losses_epoch.append(loss.item())\n",
        "\n",
        "        loss.backward()     \n",
        "          \n",
        "        optimizer.step()             \n",
        "        optimizer.zero_grad() \n",
        " \n",
        "    L1_list = []\n",
        "#     if epoch % 10 == 0:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, label in iter(validation_loader):\n",
        "            x  = x.to(device)\n",
        "            label = label.to(device)\n",
        "            out, _, _ = model(x, label)\n",
        "            L1_list.append(torch.mean(torch.abs(out-x)).item())\n",
        "            \n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] loss {np.mean(np.array(losses_epoch))}, validation L1 = {np.mean(L1_list)}\")\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "HvFWc-SVlzEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99681104-e6ec-454a-a595-79fb1c789bc3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] loss 14123.838140483786, validation L1 = 0.09207791338364284\n",
            "Epoch [2/20] loss 5510.24810306215, validation L1 = 0.10148066336788782\n",
            "Epoch [3/20] loss 6345.050029933332, validation L1 = 0.09510272238554994\n",
            "Epoch [4/20] loss 5492.674318395176, validation L1 = 0.10518599053223927\n",
            "Epoch [5/20] loss 5473.298645143839, validation L1 = 0.09052932795470323\n",
            "Epoch [6/20] loss 2911.526459175312, validation L1 = 0.07468495651231549\n",
            "Epoch [7/20] loss 3610.434245123154, validation L1 = 0.07671164866627717\n",
            "Epoch [8/20] loss 4757.163626428049, validation L1 = 0.0863970933648629\n",
            "Epoch [9/20] loss 4498.943192926784, validation L1 = 0.08984702613537873\n",
            "Epoch [10/20] loss 4509.377809357497, validation L1 = 0.08176777269539794\n",
            "Epoch [11/20] loss 2431.1147518119114, validation L1 = 0.06767891189916347\n",
            "Epoch [12/20] loss 3063.9861560829304, validation L1 = 0.07186688649339404\n",
            "Epoch [13/20] loss 4239.726305804282, validation L1 = 0.09106506074104852\n",
            "Epoch [14/20] loss 4307.136294487294, validation L1 = 0.08637934783852197\n",
            "Epoch [15/20] loss 3975.7335662531027, validation L1 = 0.0785654873503902\n",
            "Epoch [16/20] loss 2216.478374411278, validation L1 = 0.06609131429132407\n",
            "Epoch [17/20] loss 2908.168016732832, validation L1 = 0.0698177563708003\n",
            "Epoch [18/20] loss 3872.652682240286, validation L1 = 0.08033415220859574\n",
            "Epoch [19/20] loss 3848.4634360878626, validation L1 = 0.08037053496857\n",
            "Epoch [20/20] loss 3747.566369703491, validation L1 = 0.08119618432308601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ocena"
      ],
      "metadata": {
        "id": "KAVQhxQwmKa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_images(num):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for i in range(0, num):\n",
        "        r = random.randint(0, len(dataset))\n",
        "        images.append(dataset[r][0])\n",
        "        labels.append(dataset[r][1])\n",
        "    return torch.stack(images, dim=0), torch.tensor(labels)"
      ],
      "metadata": {
        "id": "QbT9AAit0WkX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_reconstructions(model, input_imgs, labels, device):\n",
        "    # Reconstruct images\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        reconst_imgs, means, log_var = model(input_imgs.to(device), labels.to(device))\n",
        "    reconst_imgs = reconst_imgs.cpu()\n",
        "    \n",
        "    # Plotting\n",
        "    imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0,1)\n",
        "    grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=False, range=(-1,1))\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    if len(input_imgs) == 4:\n",
        "        plt.figure(figsize=(10,10))\n",
        "    else:\n",
        "        plt.figure(figsize=(15,10))\n",
        "    plt.title(f\"Reconstructions\")\n",
        "    plt.imshow(grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WLRYkBof0GzX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rekonstrukcja obrazów"
      ],
      "metadata": {
        "id": "dlKWpVBNhuG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_imgs, labels = get_train_images(8)\n",
        "visualize_reconstructions(model, input_imgs, labels, device)"
      ],
      "metadata": {
        "id": "0G2RWbQ60PCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generacje"
      ],
      "metadata": {
        "id": "oYwWLSKPmLl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, n_imgs, device):\n",
        "    # Generate images\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        label = torch.tensor(random.choices(list(classes_len.keys()), weights = list(classes_len.values()), k = n_imgs))\n",
        "        generated_imgs = model.generate(torch.randn([n_imgs, model.latent_dim]).to(device), label.to(device))\n",
        "    return generated_imgs.cpu()\n"
      ],
      "metadata": {
        "id": "FRdO3xmbwlMo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images):\n",
        "    grid = torchvision.utils.make_grid(images, nrow=4, normalize=False, range=(-1,1))\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    if len(images) == 4:\n",
        "        plt.figure(figsize=(10,10))\n",
        "    else:\n",
        "        plt.figure(figsize=(15,10))\n",
        "    plt.title(f\"Generations\")\n",
        "    plt.imshow(grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0Zy5RedwLAHT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_images = generate_images(model, 16 , device)\n",
        "show_images(generated_images)"
      ],
      "metadata": {
        "id": "VpCD1EL_JPDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_images = generate_images(model, 1000 , device)\n"
      ],
      "metadata": {
        "id": "1QQaaoFkz32A"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir /content/generated_data\n",
        "!rm /content/generated_data/*\n",
        "\n",
        "for cnt, image in enumerate(generated_images):\n",
        "    save_image(image, '/content/generated_data/gen_' + str(cnt) + '.png')"
      ],
      "metadata": {
        "id": "YVRBfSY_LWvo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Liczenie wartości Fréchet Inception Distance (FID)"
      ],
      "metadata": {
        "id": "mjapaqvah-9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "id": "pQa7njYhznSi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytorch_fid --device cuda:0 '/content/validation_data' '/content/generated_data' \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L8P6DrxFgoa",
        "outputId": "ce72d73d-6669-4471-d0d6-b40782d6b7df"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 20/20 [00:04<00:00,  4.82it/s]\n",
            "100% 20/20 [00:04<00:00,  4.92it/s]\n",
            "FID:  79.51691877536854\n"
          ]
        }
      ]
    }
  ]
}